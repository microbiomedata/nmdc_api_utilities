{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End NMDC Study Analysis\n",
    "## Study: nmdc:sty-11-aygzgv51\n",
    "\n",
    "This notebook demonstrates a comprehensive analysis workflow using the NMDC API utilities:\n",
    "\n",
    "1. **Study Exploration** - Retrieve and examine study metadata\n",
    "2. **Sample Parameter Analysis** - Explore biosample characteristics (depth, pH, temperature, ecosystem types)\n",
    "3. **Data Type Discovery** - Identify available data types (functional annotations, metabolites)\n",
    "4. **Functional Annotation Analysis** - Examine EC numbers, PFAM domains, COG categories, KEGG orthologs\n",
    "5. **Enrichment Analysis** - Compare functional profiles across sample groups\n",
    "6. **Visualizations** - Create informative plots of the findings\n",
    "\n",
    "### Scientific Questions\n",
    "- What are the environmental characteristics of samples in this study?\n",
    "- What functional capabilities (enzymes, protein families) are present?\n",
    "- Are certain functions enriched in specific environmental conditions?\n",
    "- What metabolic pathways are represented across different sample types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# NMDC API utilities\n",
    "from nmdc_api_utilities.study_search import StudySearch\n",
    "from nmdc_api_utilities.biosample_search import BiosampleSearch\n",
    "from nmdc_api_utilities.data_object_search import DataObjectSearch\n",
    "\n",
    "# Set up plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Study ID\n",
    "STUDY_ID = \"nmdc:sty-11-aygzgv51\"\n",
    "\n",
    "print(f\"Analyzing NMDC Study: {STUDY_ID}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Study Exploration\n",
    "\n",
    "First, let's retrieve the study metadata and understand what this study is about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize study search client\n",
    "study_client = StudySearch(env=\"prod\")\n",
    "\n",
    "# Get study metadata\n",
    "study = study_client.get_record_by_id(collection_id=STUDY_ID)\n",
    "\n",
    "# Display key study information\n",
    "print(\"Study Information\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ID: {study.get('id', 'N/A')}\")\n",
    "print(f\"Name: {study.get('name', 'N/A')}\")\n",
    "print(f\"Description: {study.get('description', 'N/A')[:500]}...\")  # First 500 chars\n",
    "print(f\"\\nPrincipal Investigator: {study.get('principal_investigator', {}).get('has_raw_value', 'N/A')}\")\n",
    "print(f\"Ecosystem Category: {study.get('ecosystem_category', 'N/A')}\")\n",
    "print(f\"Ecosystem Type: {study.get('ecosystem_type', 'N/A')}\")\n",
    "print(f\"Ecosystem Subtype: {study.get('ecosystem_subtype', 'N/A')}\")\n",
    "\n",
    "# Check for study-level environmental context\n",
    "if 'study_category' in study:\n",
    "    print(f\"\\nStudy Category: {study['study_category']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sample Parameter Exploration\n",
    "\n",
    "Now let's retrieve all biosamples associated with this study and explore their environmental parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load biosamples from local data (API can be slow/timeout)\ntry:\n    # Try to load from downloaded study data\n    # dump-study creates individual biosample.json files in each biosample directory\n    from pathlib import Path\n    study_dir = Path('../study_data_complete')\n    biosample_files = list(study_dir.glob(\"nmdc_bsm-*/biosample.json\"))\n    \n    if not biosample_files:\n        raise FileNotFoundError(\"No biosample files found in study_data_complete\")\n    \n    print(f\"Found {len(biosample_files)} biosample files\")\n    \n    # Aggregate individual biosample files into a list\n    biosamples = []\n    for biosample_file in biosample_files:\n        with open(biosample_file) as f:\n            biosample = json.load(f)\n            biosamples.append(biosample)\n    \n    print(f\"Loaded {len(biosamples)} biosamples from local cache\")\nexcept FileNotFoundError as e:\n    # Fallback to API\n    print(f\"Local files not found: {e}\")\n    print(\"Fetching biosamples from API (this may take a while)...\")\n    biosamples = study_client.get_linked_biosamples(study_id=STUDY_ID)\n\nprint(f\"Found {len(biosamples)} biosamples in this study\")\nprint(\"=\"*60)\n\n# Display a few sample IDs\nprint(\"\\nSample IDs (first 10):\")\nfor i, bs in enumerate(biosamples[:10]):\n    print(f\"  {i+1}. {bs.get('id', 'N/A')} - {bs.get('name', 'N/A')[:50]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Environmental Parameters\n",
    "\n",
    "Let's extract and analyze key environmental parameters from the biosamples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract environmental parameters\n",
    "def extract_numeric_value(field_dict):\n",
    "    \"\"\"Extract numeric value from NMDC measurement fields.\"\"\"\n",
    "    if not field_dict:\n",
    "        return None\n",
    "    if isinstance(field_dict, dict):\n",
    "        # Try different field names\n",
    "        for key in ['has_numeric_value', 'has_maximum_numeric_value', 'has_minimum_numeric_value']:\n",
    "            if key in field_dict:\n",
    "                return field_dict[key]\n",
    "    return None\n",
    "\n",
    "def extract_raw_value(field_dict):\n",
    "    \"\"\"Extract raw value from NMDC fields.\"\"\"\n",
    "    if not field_dict:\n",
    "        return None\n",
    "    if isinstance(field_dict, dict) and 'has_raw_value' in field_dict:\n",
    "        return field_dict['has_raw_value']\n",
    "    return str(field_dict) if field_dict else None\n",
    "\n",
    "# Build a dataframe of sample parameters\n",
    "sample_data = []\n",
    "for bs in biosamples:\n",
    "    sample_info = {\n",
    "        'id': bs.get('id'),\n",
    "        'name': bs.get('name'),\n",
    "        'ecosystem_category': extract_raw_value(bs.get('ecosystem_category')),\n",
    "        'ecosystem_type': extract_raw_value(bs.get('ecosystem_type')),\n",
    "        'ecosystem_subtype': extract_raw_value(bs.get('ecosystem_subtype')),\n",
    "        'depth': extract_numeric_value(bs.get('depth')),\n",
    "        'ph': extract_numeric_value(bs.get('ph')),\n",
    "        'temp': extract_numeric_value(bs.get('temp')),\n",
    "        'lat': extract_numeric_value(bs.get('lat_lon', {}).get('latitude') if isinstance(bs.get('lat_lon'), dict) else None),\n",
    "        'lon': extract_numeric_value(bs.get('lat_lon', {}).get('longitude') if isinstance(bs.get('lat_lon'), dict) else None),\n",
    "    }\n",
    "    sample_data.append(sample_info)\n",
    "\n",
    "df_samples = pd.DataFrame(sample_data)\n",
    "\n",
    "print(f\"\\nSample DataFrame Shape: {df_samples.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(df_samples.head())\n",
    "\n",
    "print(\"\\nData Summary:\")\n",
    "display(df_samples.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Ecosystem Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ecosystem distribution\n",
    "print(\"Ecosystem Distribution\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for col in ['ecosystem_category', 'ecosystem_type', 'ecosystem_subtype']:\n",
    "    if col in df_samples.columns:\n",
    "        counts = df_samples[col].value_counts()\n",
    "        print(f\"\\n{col.replace('_', ' ').title()}:\")\n",
    "        for val, count in counts.items():\n",
    "            if val:\n",
    "                print(f\"  {val}: {count}\")\n",
    "\n",
    "# Visualize ecosystem types\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for idx, col in enumerate(['ecosystem_category', 'ecosystem_type', 'ecosystem_subtype']):\n",
    "    if col in df_samples.columns:\n",
    "        data = df_samples[col].value_counts()\n",
    "        if len(data) > 0:\n",
    "            data.plot(kind='bar', ax=axes[idx], color='steelblue')\n",
    "            axes[idx].set_title(f\"{col.replace('_', ' ').title()} Distribution\")\n",
    "            axes[idx].set_xlabel('')\n",
    "            axes[idx].set_ylabel('Count')\n",
    "            axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Environmental Parameter Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of numeric parameters\n",
    "numeric_params = ['depth', 'ph', 'temp']\n",
    "available_params = [p for p in numeric_params if p in df_samples.columns and df_samples[p].notna().sum() > 0]\n",
    "\n",
    "if available_params:\n",
    "    fig, axes = plt.subplots(1, len(available_params), figsize=(5*len(available_params), 4))\n",
    "    if len(available_params) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, param in enumerate(available_params):\n",
    "        data = df_samples[param].dropna()\n",
    "        axes[idx].hist(data, bins=20, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "        axes[idx].set_title(f\"{param.upper()} Distribution\")\n",
    "        axes[idx].set_xlabel(param.title())\n",
    "        axes[idx].set_ylabel('Frequency')\n",
    "        \n",
    "        # Add statistics\n",
    "        mean_val = data.mean()\n",
    "        median_val = data.median()\n",
    "        axes[idx].axvline(mean_val, color='red', linestyle='--', label=f'Mean: {mean_val:.2f}')\n",
    "        axes[idx].axvline(median_val, color='green', linestyle='--', label=f'Median: {median_val:.2f}')\n",
    "        axes[idx].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nEnvironmental Parameter Statistics:\")\n",
    "    print(\"=\"*60)\n",
    "    for param in available_params:\n",
    "        data = df_samples[param].dropna()\n",
    "        print(f\"\\n{param.upper()}:\")\n",
    "        print(f\"  Count: {len(data)}\")\n",
    "        print(f\"  Mean: {data.mean():.2f}\")\n",
    "        print(f\"  Median: {data.median():.2f}\")\n",
    "        print(f\"  Std Dev: {data.std():.2f}\")\n",
    "        print(f\"  Min: {data.min():.2f}\")\n",
    "        print(f\"  Max: {data.max():.2f}\")\n",
    "else:\n",
    "    print(\"No numeric environmental parameters available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Geographic Distribution\n",
    "\n",
    "If lat/lon coordinates are available, let's visualize the geographic distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot geographic distribution\n",
    "if 'lat' in df_samples.columns and 'lon' in df_samples.columns:\n",
    "    df_geo = df_samples[['lat', 'lon']].dropna()\n",
    "    \n",
    "    if len(df_geo) > 0:\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        scatter = ax.scatter(df_geo['lon'], df_geo['lat'], \n",
    "                           c=range(len(df_geo)), cmap='viridis', \n",
    "                           s=100, alpha=0.6, edgecolors='black')\n",
    "        ax.set_xlabel('Longitude', fontsize=12)\n",
    "        ax.set_ylabel('Latitude', fontsize=12)\n",
    "        ax.set_title('Geographic Distribution of Samples', fontsize=14)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.colorbar(scatter, label='Sample Index', ax=ax)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nMapped {len(df_geo)} samples with coordinates\")\n",
    "    else:\n",
    "        print(\"No samples with valid lat/lon coordinates found.\")\n",
    "else:\n",
    "    print(\"No geographic coordinates available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Type Discovery\n",
    "\n",
    "Now let's explore what types of data are available for this study. We're particularly interested in:\n",
    "- Functional annotations (from metagenomes and proteomes)\n",
    "- Metabolite data\n",
    "- GFF annotation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all data objects for the study\n",
    "print(\"Retrieving data objects for the study...\")\n",
    "print(\"This may take a minute...\\n\")\n",
    "\n",
    "try:\n",
    "    data_objects = study_client.get_all_linked_data_objects(study_id=STUDY_ID, group_by_type=True)\n",
    "except Exception as e:\n",
    "    print(f\"API request failed: {e}\")\n",
    "    print(\"Using local data from study dump...\\n\")\n",
    "    \n",
    "    # Load from local files\n",
    "    from pathlib import Path\n",
    "    data_objects = defaultdict(list)\n",
    "    \n",
    "    study_dir = Path('../my_study')\n",
    "    for biosample_dir in study_dir.glob('nmdc_bsm-*'):\n",
    "        data_dir = biosample_dir / 'data_objects'\n",
    "        if data_dir.exists():\n",
    "            # Get metadata for each file\n",
    "            for file in data_dir.glob('*.json'):\n",
    "                try:\n",
    "                    with open(file) as f:\n",
    "                        obj = json.load(f)\n",
    "                        obj_type = obj.get('data_object_type', obj.get('type', 'Unknown'))\n",
    "                        data_objects[obj_type].append(obj)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "print(f\"Found {len(data_objects)} data object types\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display data types and counts\n",
    "data_type_counts = {}\n",
    "for data_type, objects in data_objects.items():\n",
    "    count = len(objects)\n",
    "    data_type_counts[data_type] = count\n",
    "    print(f\"{data_type}: {count} files\")\n",
    "\n",
    "# Sort by count\n",
    "data_type_counts_sorted = dict(sorted(data_type_counts.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data type distribution\n",
    "if data_type_counts_sorted:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    types = list(data_type_counts_sorted.keys())\n",
    "    counts = list(data_type_counts_sorted.values())\n",
    "    \n",
    "    bars = ax.barh(types, counts, color='coral', edgecolor='black', alpha=0.7)\n",
    "    ax.set_xlabel('Number of Files', fontsize=12)\n",
    "    ax.set_ylabel('Data Object Type', fontsize=12)\n",
    "    ax.set_title('Distribution of Data Types in Study', fontsize=14)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add count labels\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        ax.text(width, bar.get_y() + bar.get_height()/2, \n",
    "               f'{int(width)}', ha='left', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Focus on Functional Annotations\n",
    "\n",
    "Let's identify data types related to functional annotations (EC numbers, PFAM, COG, KEGG, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords for functional annotation files\n",
    "annotation_keywords = ['annotation', 'ec', 'pfam', 'cog', 'kegg', 'ko', 'gff', 'ortholog', \n",
    "                       'cath', 'smart', 'tigrfam', 'superfamily', 'functional']\n",
    "\n",
    "# Filter for annotation-related data types\n",
    "annotation_types = {}\n",
    "for data_type, objects in data_objects.items():\n",
    "    if any(keyword in data_type.lower() for keyword in annotation_keywords):\n",
    "        annotation_types[data_type] = objects\n",
    "\n",
    "print(\"Functional Annotation Data Types:\")\n",
    "print(\"=\"*60)\n",
    "for data_type, objects in annotation_types.items():\n",
    "    print(f\"\\n{data_type}: {len(objects)} files\")\n",
    "    # Show a few example file names\n",
    "    for obj in objects[:3]:\n",
    "        print(f\"  - {obj.get('name', 'N/A')[:80]}\")\n",
    "    if len(objects) > 3:\n",
    "        print(f\"  ... and {len(objects)-3} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Metabolite and Chemical Data\n",
    "\n",
    "Let's check for metabolite data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords for metabolite/chemical data\n",
    "metabolite_keywords = ['metabol', 'chemical', 'compound', 'lipidomic', 'metabolomic', \n",
    "                       'organic', 'natural products', 'mass spec', 'ms', 'nmr']\n",
    "\n",
    "# Filter for metabolite-related data types\n",
    "metabolite_types = {}\n",
    "for data_type, objects in data_objects.items():\n",
    "    if any(keyword in data_type.lower() for keyword in metabolite_keywords):\n",
    "        metabolite_types[data_type] = objects\n",
    "\n",
    "if metabolite_types:\n",
    "    print(\"Metabolite/Chemical Data Types:\")\n",
    "    print(\"=\"*60)\n",
    "    for data_type, objects in metabolite_types.items():\n",
    "        print(f\"\\n{data_type}: {len(objects)} files\")\n",
    "        for obj in objects[:5]:\n",
    "            print(f\"  - {obj.get('name', 'N/A')[:80]}\")\n",
    "        if len(objects) > 5:\n",
    "            print(f\"  ... and {len(objects)-5} more\")\n",
    "else:\n",
    "    print(\"No metabolite data found for this study.\")\n",
    "    print(\"This study appears to focus on genomic/metagenomic data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Functional Annotation Analysis\n",
    "\n",
    "Now let's analyze the functional annotations in detail. We'll look for patterns in:\n",
    "- EC numbers (enzyme functions)\n",
    "- PFAM domains (protein families)\n",
    "- COG categories (functional categories)\n",
    "- KEGG orthologs (metabolic pathways)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Sample Annotation Files\n",
    "\n",
    "Let's examine some annotation files to understand their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find GFF files or annotation TSV files\n",
    "gff_files = data_objects.get('Annotation GFF', [])\n",
    "ec_files = data_objects.get('Annotation Enzyme Commission', [])\n",
    "ko_files = data_objects.get('Annotation KEGG Orthology', [])\n",
    "pfam_files = data_objects.get('Annotation PFAM', [])\n",
    "cog_files = data_objects.get('Annotation COG', [])\n",
    "\n",
    "print(\"Available Annotation Files:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"GFF files: {len(gff_files)}\")\n",
    "print(f\"EC annotation files: {len(ec_files)}\")\n",
    "print(f\"KEGG Orthology files: {len(ko_files)}\")\n",
    "print(f\"PFAM files: {len(pfam_files)}\")\n",
    "print(f\"COG files: {len(cog_files)}\")\n",
    "\n",
    "# Show example file URLs (if available)\n",
    "if gff_files:\n",
    "    print(\"\\nExample GFF file:\")\n",
    "    print(f\"  ID: {gff_files[0].get('id')}\")\n",
    "    print(f\"  Name: {gff_files[0].get('name')}\")\n",
    "    if 'url' in gff_files[0]:\n",
    "        print(f\"  URL: {gff_files[0]['url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Aggregate Functional Profile\n",
    "\n",
    "Since individual annotation files can be large, let's create an aggregate view of functional categories across the study.\n",
    "\n",
    "**Note**: This is a demonstration. For real analysis, you would download and parse the annotation files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary of available functional data\n",
    "functional_summary = {\n",
    "    'Total Biosamples': len(biosamples),\n",
    "    'GFF Annotation Files': len(gff_files),\n",
    "    'EC Annotation Files': len(ec_files),\n",
    "    'KEGG Orthology Files': len(ko_files),\n",
    "    'PFAM Files': len(pfam_files),\n",
    "    'COG Files': len(cog_files),\n",
    "}\n",
    "\n",
    "print(\"Functional Annotation Summary:\")\n",
    "print(\"=\"*60)\n",
    "for key, value in functional_summary.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Calculate coverage (% of samples with each annotation type)\n",
    "if len(biosamples) > 0:\n",
    "    print(\"\\nAnnotation Coverage:\")\n",
    "    print(\"=\"*60)\n",
    "    for ann_type, count in [(\"EC\", len(ec_files)), (\"KO\", len(ko_files)), \n",
    "                             (\"PFAM\", len(pfam_files)), (\"COG\", len(cog_files))]:\n",
    "        coverage = (count / len(biosamples)) * 100\n",
    "        print(f\"{ann_type}: {coverage:.1f}% of samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Enrichment Analysis\n",
    "\n",
    "For enrichment analysis, we would typically:\n",
    "1. Download the study data using `nmdc dump-study`\n",
    "2. Run enrichment analysis using `nmdc enrich`\n",
    "\n",
    "Let's demonstrate the workflow and what questions we could answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Enrichment Analysis Strategy\n",
    "\n",
    "Based on the sample parameters we explored, here are interesting enrichment analyses we could perform:\n",
    "\n",
    "#### If depth data is available:\n",
    "```bash\n",
    "# Compare shallow vs deep samples\n",
    "nmdc enrich ./study_data --group-by depth --threshold <median_depth> --annotation-type ec_number\n",
    "```\n",
    "\n",
    "#### If pH data is available:\n",
    "```bash\n",
    "# Compare acidic vs neutral/alkaline samples\n",
    "nmdc enrich ./study_data --group-by ph --threshold 7.0 --annotation-type pfam\n",
    "```\n",
    "\n",
    "#### If multiple ecosystem types:\n",
    "```bash\n",
    "# Compare specific ecosystem types\n",
    "nmdc enrich ./study_data --group-by ecosystem_type --categories \"Soil,Marine\" --annotation-type ko\n",
    "```\n",
    "\n",
    "#### If temperature data:\n",
    "```bash\n",
    "# Compare cold vs warm environments\n",
    "nmdc enrich ./study_data --group-by temp --bins 2 --annotation-type cog\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate enrichment analysis recommendations based on available parameters\n",
    "print(\"Recommended Enrichment Analyses:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Check for continuous variables\n",
    "if 'depth' in df_samples.columns and df_samples['depth'].notna().sum() > 5:\n",
    "    median_depth = df_samples['depth'].median()\n",
    "    recommendations.append({\n",
    "        'name': 'Depth-based enrichment',\n",
    "        'rationale': f'Compare functions in shallow (≤{median_depth:.1f}) vs deep (>{median_depth:.1f}) samples',\n",
    "        'command': f'nmdc enrich ./study_data --group-by depth --threshold {median_depth:.1f} --annotation-type ec_number',\n",
    "        'question': 'Are certain enzymes enriched at different depths?'\n",
    "    })\n",
    "\n",
    "if 'ph' in df_samples.columns and df_samples['ph'].notna().sum() > 5:\n",
    "    median_ph = df_samples['ph'].median()\n",
    "    recommendations.append({\n",
    "        'name': 'pH-based enrichment',\n",
    "        'rationale': f'Compare functions in acidic (≤{median_ph:.1f}) vs alkaline (>{median_ph:.1f}) samples',\n",
    "        'command': f'nmdc enrich ./study_data --group-by ph --threshold {median_ph:.1f} --annotation-type pfam',\n",
    "        'question': 'Which protein families are associated with pH tolerance?'\n",
    "    })\n",
    "\n",
    "if 'temp' in df_samples.columns and df_samples['temp'].notna().sum() > 5:\n",
    "    recommendations.append({\n",
    "        'name': 'Temperature-based enrichment',\n",
    "        'rationale': 'Compare functions in cold vs warm environments',\n",
    "        'command': 'nmdc enrich ./study_data --group-by temp --bins 2 --annotation-type cog',\n",
    "        'question': 'What COG categories are temperature-dependent?'\n",
    "    })\n",
    "\n",
    "# Check for categorical variables\n",
    "if 'ecosystem_type' in df_samples.columns:\n",
    "    eco_types = df_samples['ecosystem_type'].value_counts()\n",
    "    if len(eco_types) >= 2:\n",
    "        top_two = ','.join(eco_types.head(2).index.tolist())\n",
    "        recommendations.append({\n",
    "            'name': 'Ecosystem type comparison',\n",
    "            'rationale': f'Compare functions between {top_two}',\n",
    "            'command': f'nmdc enrich ./study_data --group-by ecosystem_type --categories \"{top_two}\" --annotation-type ko',\n",
    "            'question': 'What metabolic pathways differ between ecosystem types?'\n",
    "        })\n",
    "\n",
    "# Display recommendations\n",
    "if recommendations:\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"\\n{i}. {rec['name']}\")\n",
    "        print(f\"   Scientific Question: {rec['question']}\")\n",
    "        print(f\"   Rationale: {rec['rationale']}\")\n",
    "        print(f\"   Command: {rec['command']}\")\n",
    "else:\n",
    "    print(\"\\nInsufficient sample parameters for enrichment analysis.\")\n",
    "    print(\"Need at least 5 samples with numeric or categorical metadata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Real Enrichment Results\n",
    "\n",
    "We've performed actual enrichment analysis comparing samples from different environmental media (ENVO:01000017 vs ENVO:00002007).\n",
    "This analysis used real functional annotation data from GFF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load real enrichment results\n# Try multiple possible locations for enrichment results\nenrichment_paths = [\n    '../study_data_complete/enrichment_env_medium_ec.tsv',\n    'enrichment_env_medium_ec.tsv',\n    'enrichment_with_all_labels.tsv',\n    '../enrichment_env_medium_ec.tsv'\n]\n\nenrichment_file = None\nfor path in enrichment_paths:\n    if Path(path).exists():\n        enrichment_file = path\n        break\n\nif enrichment_file:\n    try:\n        enrichment_df = pd.read_csv(enrichment_file, sep='\\t')\n        \n        print(\"Real Enrichment Analysis Results\")\n        print(f\"Loaded from: {enrichment_file}\")\n        print(f\"Comparing: {enrichment_df['group1_name'].iloc[0]} vs {enrichment_df['group2_name'].iloc[0]}\")\n        print(\"=\"*60)\n        print(f\"Total features tested: {len(enrichment_df)}\")\n        \n        # Filter for significant results\n        significant = enrichment_df[enrichment_df['fdr'] < 0.05]\n        print(f\"Significant features (FDR < 0.05): {len(significant)}\")\n        \n        # Show top 20 most significant\n        print(\"\\nTop 20 Most Significant Features:\")\n        print(\"=\"*60)\n        display_cols = ['feature_id', 'group1_name', 'group1_count', \n                        'group2_name', 'group2_count', 'p_value', \n                        'fdr', 'effect_size', 'enriched_in']\n        # Add feature_name if available (from OAKlib labeling)\n        if 'feature_name' in enrichment_df.columns:\n            display_cols.insert(1, 'feature_name')\n        \n        top20 = enrichment_df.head(20)[display_cols]\n        display(top20)\n        \n        # Visualize enrichment\n        if len(significant) > 0:\n            # Prepare data for plotting\n            plot_data = enrichment_df.head(100).copy()  # Top 100 for visualization\n            plot_data['neg_log_pval'] = -np.log10(plot_data['p_value'])\n            plot_data['log2_fc'] = np.log2(plot_data['effect_size'])\n            \n            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n            \n            # Volcano plot\n            colors = ['red' if fdr < 0.05 else 'gray' for fdr in plot_data['fdr']]\n            ax1.scatter(plot_data['log2_fc'], plot_data['neg_log_pval'], \n                       c=colors, s=60, alpha=0.6, edgecolors='black', linewidth=0.5)\n            ax1.axhline(-np.log10(0.05), color='blue', linestyle='--', \n                       linewidth=2, label='p=0.05', alpha=0.7)\n            ax1.axvline(0, color='black', linestyle='-', linewidth=0.5)\n            ax1.set_xlabel('Log2(Fold Change)', fontsize=12, fontweight='bold')\n            ax1.set_ylabel('-Log10(p-value)', fontsize=12, fontweight='bold')\n            ax1.set_title('Enrichment Volcano Plot (Top 100)', fontsize=14, fontweight='bold')\n            ax1.legend()\n            ax1.grid(True, alpha=0.3)\n            \n            # Bar plot of top enriched features (10 in each direction)\n            group1_enriched = significant[significant['enriched_in'] == significant['group1_name'].iloc[0]].head(10)\n            group2_enriched = significant[significant['enriched_in'] == significant['group2_name'].iloc[0]].head(10)\n            \n            # Combine and sort by fold change\n            top_features = pd.concat([group1_enriched, group2_enriched]).sort_values('effect_size')\n            \n            colors_bar = ['steelblue' if x == top_features['group1_name'].iloc[0] \n                         else 'coral' for x in top_features['enriched_in']]\n            \n            # Use feature_name for labels if available, otherwise feature_id\n            if 'feature_name' in top_features.columns:\n                labels = [f\"{row['feature_id']}\\n{row['feature_name'][:40]}\" \n                         for _, row in top_features.iterrows()]\n            else:\n                labels = top_features['feature_id'].tolist()\n            \n            ax2.barh(range(len(top_features)), top_features['effect_size'], \n                    color=colors_bar, edgecolor='black', alpha=0.7, linewidth=0.5)\n            ax2.set_yticks(range(len(top_features)))\n            ax2.set_yticklabels(labels, fontsize=8)\n            ax2.axvline(1, color='black', linestyle='--', linewidth=1)\n            ax2.set_xlabel('Fold Change', fontsize=12, fontweight='bold')\n            ax2.set_ylabel('Feature', fontsize=12, fontweight='bold')\n            ax2.set_title('Top Enriched Features (10 per group)', fontsize=14, fontweight='bold')\n            ax2.set_xscale('log')\n            ax2.grid(axis='x', alpha=0.3)\n            \n            # Add legend\n            from matplotlib.patches import Patch\n            legend_elements = [\n                Patch(facecolor='steelblue', label=top_features['group1_name'].iloc[0]),\n                Patch(facecolor='coral', label=top_features['group2_name'].iloc[0])\n            ]\n            ax2.legend(handles=legend_elements, loc='best')\n            \n            plt.tight_layout()\n            plt.show()\n            \n            # Summary statistics\n            print(f\"\\n\" + \"=\"*60)\n            print(\"Enrichment Summary:\")\n            print(f\"  Features enriched in {enrichment_df['group1_name'].iloc[0]}: \"\n                  f\"{len(significant[significant['enriched_in'] == enrichment_df['group1_name'].iloc[0]])}\")\n            print(f\"  Features enriched in {enrichment_df['group2_name'].iloc[0]}: \"\n                  f\"{len(significant[significant['enriched_in'] == enrichment_df['group2_name'].iloc[0]])}\")\n            \n            # Show some interesting highly enriched features\n            print(\"\\nHighly Enriched Features (Fold Change > 10):\")\n            highly_enriched = significant[significant['effect_size'] > 10].head(10)\n            if len(highly_enriched) > 0:\n                display_cols_he = ['feature_id', 'effect_size', 'enriched_in']\n                if 'feature_name' in highly_enriched.columns:\n                    display_cols_he.insert(1, 'feature_name')\n                for _, row in highly_enriched.iterrows():\n                    if 'feature_name' in row:\n                        print(f\"  {row['feature_id']} ({row['feature_name']}): {row['effect_size']:.1f}x enriched in {row['enriched_in']}\")\n                    else:\n                        print(f\"  {row['feature_id']}: {row['effect_size']:.1f}x enriched in {row['enriched_in']}\")\n            else:\n                print(\"  (None with fold change > 10)\")\n                \n    except Exception as e:\n        print(f\"Error loading enrichment results: {e}\")\n        import traceback\n        traceback.print_exc()\nelse:\n    print(\"Enrichment results file not found in any of the expected locations:\")\n    for path in enrichment_paths:\n        print(f\"  - {path}\")\n    print(\"\\nTo generate real enrichment results, run:\")\n    print(\"  cd notebooks\")\n    print(\"  nmdc enrich study_data_complete --group-by env_medium --categories 'ENVO:01000017,ENVO:00002007' --annotation-type ec_number --output enrichment_env_medium_ec.tsv\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Next Steps\n",
    "\n",
    "### What We Discovered\n",
    "\n",
    "1. **Study Characteristics**: Retrieved study metadata and context\n",
    "2. **Sample Parameters**: Analyzed environmental parameters across biosamples\n",
    "3. **Data Availability**: Identified available functional annotation data types\n",
    "4. **Enrichment Potential**: Identified opportunities for comparative functional analysis\n",
    "\n",
    "### Next Steps for Real Analysis\n",
    "\n",
    "To perform a complete analysis with real data:\n",
    "\n",
    "```bash\n",
    "# 1. Download the complete study data\n",
    "nmdc dump-study nmdc:sty-11-aygzgv51 ./study_data --download-data \\\n",
    "  --include-types \"Annotation GFF,Annotation Enzyme Commission,Annotation KEGG Orthology\"\n",
    "\n",
    "# 2. Run enrichment analysis (use one of the recommended analyses above)\n",
    "nmdc enrich ./study_data --group-by depth --threshold 10.0 \\\n",
    "  --annotation-type ec_number --output enrichment_results.tsv\n",
    "\n",
    "# 3. Analyze GFF files for specific functions\n",
    "nmdc gff query ./study_data/biosample_*/data_objects/*.gff \\\n",
    "  --ec \"2.7.%\" --output kinases.tsv\n",
    "\n",
    "# 4. Find biosynthetic gene clusters\n",
    "nmdc gff find-bgc ./study_data/biosample_*/data_objects/*.gff \\\n",
    "  --min-genes 5 --output bgc_candidates.json\n",
    "```\n",
    "\n",
    "### Scientific Questions to Explore\n",
    "\n",
    "- How does microbial functional diversity vary with environmental parameters?\n",
    "- What metabolic pathways are enriched in specific ecosystem types?\n",
    "- Are there depth-dependent or pH-dependent functional adaptations?\n",
    "- What biosynthetic gene clusters are present and how are they distributed?\n",
    "- Can we identify ecosystem-specific enzymes or protein families?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary statistics\n",
    "print(\"Analysis Summary\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Study ID: {STUDY_ID}\")\n",
    "print(f\"Total Biosamples: {len(biosamples)}\")\n",
    "print(f\"Total Data Object Types: {len(data_objects)}\")\n",
    "print(f\"Functional Annotation Types: {len(annotation_types)}\")\n",
    "if metabolite_types:\n",
    "    print(f\"Metabolite Data Types: {len(metabolite_types)}\")\n",
    "\n",
    "# Parameter coverage\n",
    "print(\"\\nParameter Coverage:\")\n",
    "for param in ['depth', 'ph', 'temp', 'lat', 'lon']:\n",
    "    if param in df_samples.columns:\n",
    "        count = df_samples[param].notna().sum()\n",
    "        pct = (count / len(df_samples)) * 100\n",
    "        print(f\"  {param}: {count}/{len(df_samples)} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Analysis complete!\")\n",
    "print(\"\\nFor full analysis with downloaded data, use the commands shown above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Resources\n",
    "\n",
    "- NMDC Portal: https://microbiomedata.org/\n",
    "- NMDC API Documentation: https://api.microbiomedata.org/docs\n",
    "- nmdc_api_utilities Documentation: https://microbiomedata.github.io/nmdc_api_utilities/\n",
    "- Enrichment Analysis Guide: See `ENRICHMENT_ANALYSIS.md` in the repository\n",
    "- GFF Utilities Guide: See `GFF_UTILITIES.md` in the repository"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}